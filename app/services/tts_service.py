"""
Service de Synth√®se Vocale Wolof avec ADIA TTS
============================================
Service optimis√© pour la g√©n√©ration audio en wolof avec:
- Traitement par chunks pour am√©liorer la clart√©
- G√©n√©ration parall√®le pour optimiser les performances
- Support des formats web (MP3, WAV)
- Configuration avanc√©e pour la qualit√© audio
"""

import torch
import asyncio
import logging
import hashlib
import os
import re
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time

# Audio processing
import soundfile as sf
import numpy as np
from pydub import AudioSegment
from pydub.effects import normalize

# TTS model
from parler_tts import ParlerTTSForConditionalGeneration
from transformers import AutoTokenizer

logger = logging.getLogger(__name__)

# Avoid tokenizers parallelism warning when forking worker processes
# See: https://github.com/huggingface/tokenizers/issues/192
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

class AdiaWolofTTSService:
    """
    Service de synth√®se vocale Wolof avec ADIA TTS
    Optimis√© pour les textes longs avec traitement par chunks
    """
    
    def __init__(self, 
                 model_name: str = "CONCREE/Adia_TTS",
                 audio_output_dir: str = "data/audio_output",
                 max_chunk_length: int = 150,
                 overlap_words: int = 3):
        """
        Initialiser le service TTS

        Args:
            model_name: Nom du mod√®le Hugging Face
            audio_output_dir: Dossier de sortie pour les fichiers audio
            max_chunk_length: Longueur maximale d'un chunk (en caract√®res)
            overlap_words: Nombre de mots de chevauchement entre chunks
        """
        self.model_name = model_name
        self.audio_output_dir = Path(audio_output_dir)
        self.max_chunk_length = max_chunk_length
        self.overlap_words = overlap_words

        # Configuration GPU/CPU
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        logger.info(f"TTS Service initialized on device: {self.device}")

        # Mod√®le et tokenizer (chargement lazy)
        self.model = None
        self.tokenizer = None
        self._model_lock = threading.Lock()
        self._is_loading = False

        # Configuration de g√©n√©ration optimis√©e
        self.generation_config = {
            "temperature": 0.8,
            "min_new_tokens": 50,      # Minimum plus √©lev√© pour assurer la lecture compl√®te
            "max_new_tokens": 1000,    # Augment√© pour les textes plus longs
            "do_sample": True,
            "top_k": 50,
            "repetition_penalty": 1.2,
            "pad_token_id": None,      # Sera d√©fini apr√®s le chargement du tokenizer
            "eos_token_id": None,      # Sera d√©fini apr√®s le chargement du tokenizer
            "early_stopping": False    # D√©sactiv√© par d√©faut pour lecture compl√®te
        }
        
        # Configuration audio
        self.audio_config = {
            "sample_rate": 24000,  # Sera d√©fini apr√®s le chargement du mod√®le
            "output_format": "wav",
            "normalize_audio": True,
            "fade_duration": 0.1  # Fondu entre chunks (en secondes)
        }
        
        # Description vocale par d√©faut
        self.default_voice_description = "A clear and educational voice, with a flow adapted to learning"
        
        # Cr√©er le dossier de sortie
        self.audio_output_dir.mkdir(parents=True, exist_ok=True)
        
        # Cache pour √©viter la r√©g√©n√©ration
        self._audio_cache = {}
        self._cache_lock = threading.Lock()
        
        # Eviter de lancer plusieurs g√©n√©rations simultan√©es pour le m√™me texte
        # cl√© -> asyncio.Future
        self._in_progress: Dict[str, Any] = {}
        self._in_progress_lock = threading.Lock()
    
    async def load_model(self) -> bool:
        """Charger le mod√®le ADIA TTS de mani√®re asynchrone"""
        if self.model is not None and self.tokenizer is not None:
            return True
        
        with self._model_lock:
            if self._is_loading:
                # Attendre que le chargement en cours se termine
                while self._is_loading:
                    await asyncio.sleep(0.1)
                return self.model is not None
            
            self._is_loading = True
        
        try:
            logger.info(f"üîÑ Loading ADIA TTS model: {self.model_name}")
            
            # Charger le mod√®le et le tokenizer
            loop = asyncio.get_event_loop()
            
            # Chargement asynchrone du mod√®le
            self.model = await loop.run_in_executor(
                None,
                lambda: ParlerTTSForConditionalGeneration.from_pretrained(self.model_name).to(self.device)
            )
            
            # Chargement asynchrone du tokenizer
            self.tokenizer = await loop.run_in_executor(
                None,
                lambda: AutoTokenizer.from_pretrained(self.model_name)
            )
            
            # Mettre √† jour la configuration
            self.audio_config["sample_rate"] = self.model.config.sampling_rate
            # Configurer les tokens sp√©ciaux
            self.generation_config["pad_token_id"] = self.tokenizer.pad_token_id
            if hasattr(self.tokenizer, 'eos_token_id') and self.tokenizer.eos_token_id is not None:
                self.generation_config["eos_token_id"] = self.tokenizer.eos_token_id
            
            logger.info(f"‚úÖ ADIA TTS model loaded successfully")
            logger.info(f"   Device: {self.device}")
            logger.info(f"   Sample rate: {self.audio_config['sample_rate']} Hz")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load ADIA TTS model: {str(e)}")
            return False
        finally:
            self._is_loading = False
    
    def _split_text_into_chunks(self, text: str) -> List[str]:
        """
        Diviser le texte en chunks optimaux pour la synth√®se vocale
        
        Args:
            text: Texte √† diviser
            
        Returns:
            Liste de chunks de texte
        """
        if len(text) <= self.max_chunk_length:
            return [text.strip()]
        
        chunks = []
        
        # Diviser par phrases d'abord
        sentences = re.split(r'[.!?]+', text)
        
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # Si ajouter cette phrase d√©passerait la limite
            if len(current_chunk) + len(sentence) + 1 > self.max_chunk_length:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = sentence
                else:
                    # La phrase est trop longue, la diviser par mots
                    words = sentence.split()
                    word_chunk = ""
                    
                    for word in words:
                        if len(word_chunk) + len(word) + 1 > self.max_chunk_length:
                            if word_chunk:
                                chunks.append(word_chunk.strip())
                                word_chunk = word
                            else:
                                # Mot unique tr√®s long, le garder tel quel
                                chunks.append(word)
                        else:
                            word_chunk += " " + word if word_chunk else word
                    
                    if word_chunk:
                        current_chunk = word_chunk
            else:
                current_chunk += ". " + sentence if current_chunk else sentence
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        logger.info(f"üìù Text split into {len(chunks)} chunks")
        return chunks
    
    def _validate_audio_duration(self, audio_data: np.ndarray, text: str, chunk_index: int) -> bool:
        """
        Valider que la dur√©e de l'audio est coh√©rente avec le texte
        
        Args:
            audio_data: Donn√©es audio g√©n√©r√©es
            text: Texte original
            chunk_index: Index du chunk
            
        Returns:
            True si la dur√©e semble correcte
        """
        if len(audio_data) == 0:
            return False
        
        duration_seconds = len(audio_data) / self.audio_config["sample_rate"]
        char_count = len(text)
        
        # Estimation: ~10-15 caract√®res par seconde pour une lecture normale
        expected_min_duration = char_count / 15  # Lecture rapide
        expected_max_duration = char_count / 8   # Lecture lente
        
        is_valid = expected_min_duration <= duration_seconds <= expected_max_duration * 2
        
        if not is_valid:
            logger.warning(f"‚ö†Ô∏è Chunk {chunk_index}: Duration {duration_seconds:.2f}s seems incorrect for {char_count} chars")
            logger.warning(f"   Expected: {expected_min_duration:.2f}s - {expected_max_duration:.2f}s")
        else:
            logger.debug(f"‚úÖ Chunk {chunk_index}: Duration {duration_seconds:.2f}s OK for {char_count} chars")
        
        return is_valid

    def _generate_chunk_audio(self, chunk_text: str, chunk_index: int, voice_description: str, context: str = "") -> Tuple[int, np.ndarray]:
        """
        G√©n√©rer l'audio pour un chunk sp√©cifique avec une dur√©e adapt√©e au texte
        Le param√®tre `context` permet de fournir les derniers mots du chunk pr√©c√©dent
        comme contexte (sans les inclure dans le prompt principal) afin d'√©viter
        que des mots soient coup√©s en fin de chunk.
        """
        start_ts = time.time()
        try:
            logger.debug(f"üé§ Generating audio for chunk {chunk_index}: '{chunk_text[:80]}...' (context='{context[:40]}')")

            # Estimer le nombre de tokens audio n√©cessaires bas√© sur la longueur du texte
            # Approximation plus g√©n√©reuse: ~5-7 tokens audio par caract√®re de texte
            char_count = len(chunk_text)
            estimated_tokens = min(max(char_count * 6, 100), 1000)  # Plus g√©n√©reux
            min_tokens = min(char_count * 3, estimated_tokens // 2)  # Minimum plus √©lev√©

            # Pr√©parer l'input de contexte (voix + derniers mots du chunk pr√©c√©dent)
            if context:
                input_text = f"{voice_description} {context}"
            else:
                input_text = voice_description

            # Tokenization
            input_ids = self.tokenizer(input_text, return_tensors="pt").input_ids.to(self.device)
            prompt_ids = self.tokenizer(chunk_text, return_tensors="pt").input_ids.to(self.device)

            # Configuration de g√©n√©ration adapt√©e pour ce chunk
            generation_config = self.generation_config.copy()
            generation_config["max_new_tokens"] = estimated_tokens
            generation_config["min_new_tokens"] = max(min_tokens, 50)  # Minimum garanti
            generation_config["early_stopping"] = False  # D√©sactiver l'arr√™t pr√©coce

            logger.debug(f"üìä Chunk {chunk_index}: {char_count} chars ‚Üí {generation_config['min_new_tokens']}-{generation_config['max_new_tokens']} tokens")

            # G√©n√©ration audio
            with torch.no_grad():
                audio = self.model.generate(
                    input_ids=input_ids,
                    prompt_input_ids=prompt_ids,
                    **generation_config
                )

            # Convertir en numpy array
            audio_np = audio.cpu().numpy().squeeze()

            # Valider que la dur√©e semble correcte
            self._validate_audio_duration(audio_np, chunk_text, chunk_index)

            elapsed = time.time() - start_ts
            logger.info(f"‚úÖ Chunk {chunk_index} generated: {len(audio_np)} samples ({len(audio_np)/self.audio_config['sample_rate']:.2f}s) in {elapsed:.2f}s")
            return chunk_index, audio_np

        except Exception as e:
            logger.error(f"‚ùå Failed to generate audio for chunk {chunk_index}: {str(e)}")
            raise
    
    async def _generate_chunks_parallel(self, chunks: List[str], voice_description: str, max_workers: int = 3) -> List[np.ndarray]:
        """
        G√©n√©rer l'audio pour tous les chunks en parall√®le
        
        Args:
            chunks: Liste des chunks de texte
            voice_description: Description de la voix
            max_workers: Nombre maximum de workers parall√®les
            
        Returns:
            Liste des arrays audio dans l'ordre
        """
        if not self.model or not self.tokenizer:
            raise ValueError("Model not loaded. Call load_model() first.")
        
        logger.info(f"üîÑ Generating audio for {len(chunks)} chunks in parallel (max_workers={max_workers})")
        
        # Limiter le nombre de workers selon le dispositif et les ressources
        if self.device.startswith("cuda"):
            max_workers = min(max_workers, 2)  # √âviter l'overload GPU
        
        loop = asyncio.get_event_loop()
        audio_results = {}
        total = len(chunks)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Soumettre toutes les t√¢ches
            future_to_index = {}
            for i, chunk in enumerate(chunks):
                # Calculer le contexte (derniers mots du chunk pr√©c√©dent)
                context = ""
                if i > 0 and self.overlap_words > 0:
                    prev_words = chunks[i-1].split()
                    if prev_words:
                        context = " ".join(prev_words[-self.overlap_words:])
                fut = loop.run_in_executor(
                    executor,
                    self._generate_chunk_audio,
                    chunk,
                    i,
                    voice_description,
                    context
                )
                future_to_index[fut] = i

            # Collecter les r√©sultats
            completed = 0
            for future in asyncio.as_completed(list(future_to_index.keys())):
                try:
                    chunk_index, audio_data = await future
                    audio_results[chunk_index] = audio_data
                    completed += 1
                    logger.info(f"üî∏ Progress: {completed}/{total} chunks completed ({completed/total*100:.1f}%)")
                except Exception as e:
                    chunk_index = future_to_index.get(future, None)
                    logger.error(f"‚ùå Chunk {chunk_index} failed: {str(e)}")
                    raise
        
        # R√©ordonner les r√©sultats
        ordered_audio = [audio_results[i] for i in range(len(chunks))]
        logger.info(f"‚úÖ All chunks generated successfully")
        
        return ordered_audio
    
    def _trim_silence(self, audio: np.ndarray, threshold: float = 0.01) -> np.ndarray:
        """
        Supprimer les silences en d√©but et fin d'audio
        
        Args:
            audio: Array audio
            threshold: Seuil de silence (amplitude relative)
            
        Returns:
            Audio sans silences aux extr√©mit√©s
        """
        if len(audio) == 0:
            return audio
            
        # Calculer l'amplitude absolue
        abs_audio = np.abs(audio)
        
        # Trouver le premier √©chantillon non-silence
        start_idx = 0
        for i, sample in enumerate(abs_audio):
            if sample > threshold:
                start_idx = i
                break
        
        # Trouver le dernier √©chantillon non-silence
        end_idx = len(audio)
        for i in range(len(abs_audio) - 1, -1, -1):
            if abs_audio[i] > threshold:
                end_idx = i + 1
                break
        
        return audio[start_idx:end_idx]

    def _concatenate_audio_chunks(self, audio_chunks: List[np.ndarray]) -> np.ndarray:
        """
        Concat√©ner les chunks audio en supprimant les silences et sans pause
        
        Args:
            audio_chunks: Liste des arrays audio
            
        Returns:
            Array audio concat√©n√© sans bruits
        """
        if not audio_chunks:
            return np.array([])
        
        if len(audio_chunks) == 1:
            return self._trim_silence(audio_chunks[0])
        
        logger.info(f"üîó Concatenating {len(audio_chunks)} audio chunks")
        
        # Nettoyer le premier chunk
        result = self._trim_silence(audio_chunks[0])
        
        for i, chunk in enumerate(audio_chunks[1:], 1):
            # Nettoyer chaque chunk individuellement
            clean_chunk = self._trim_silence(chunk)
            
            if len(clean_chunk) == 0:
                logger.warning(f"‚ö†Ô∏è Chunk {i} is empty after trimming")
                continue
            
            # Concat√©nation directe sans fondu pour √©viter tout bruit
            result = np.concatenate([result, clean_chunk])
            
            logger.debug(f"‚úÖ Chunk {i} concatenated (trimmed)")
        
        logger.info(f"‚úÖ Audio concatenation completed: {len(result)} samples")
        return result
    
    def _save_audio_file(self, audio_data: np.ndarray, output_path: Path, format: str = "wav") -> bool:
        """
        Sauvegarder le fichier audio avec normalisation
        
        Args:
            audio_data: Donn√©es audio
            output_path: Chemin de sortie
            format: Format de sortie (wav, mp3)
            
        Returns:
            True si succ√®s
        """
        try:
            # Normaliser l'audio si demand√©
            if self.audio_config["normalize_audio"]:
                # √âviter la saturation
                max_val = np.max(np.abs(audio_data))
                if max_val > 0:
                    audio_data = audio_data / max_val * 0.95
            
            # Sauvegarder en WAV d'abord
            wav_path = output_path.with_suffix('.wav')
            sf.write(
                wav_path, 
                audio_data, 
                self.audio_config["sample_rate"],
                subtype='PCM_16'
            )
            
            logger.info(f"üíæ Audio saved: {wav_path}")
            
            # Convertir en MP3 si demand√©
            if format.lower() == "mp3":
                mp3_path = output_path.with_suffix('.mp3')
                audio_segment = AudioSegment.from_wav(str(wav_path))
                audio_segment = normalize(audio_segment)
                audio_segment.export(str(mp3_path), format="mp3", bitrate="128k")
                
                logger.info(f"üéµ MP3 version created: {mp3_path}")
                
                # Supprimer le WAV temporaire si on ne veut que le MP3
                if output_path.suffix.lower() == '.mp3':
                    wav_path.unlink()
                    return True
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to save audio file: {str(e)}")
            return False
    
    def _get_cache_key(self, text: str, voice_description: str) -> str:
        """G√©n√©rer une cl√© de cache pour le texte et la voix"""
        content = f"{text}_{voice_description}_{self.max_chunk_length}"
        return hashlib.md5(content.encode()).hexdigest()
    
    async def generate_wolof_audio(self, 
                                 wolof_text: str, 
                                 output_filename: Optional[str] = None,
                                 voice_description: Optional[str] = None,
                                 format: str = "mp3",
                                 use_cache: bool = True) -> Dict[str, Any]:
        """
        G√©n√©rer l'audio pour un texte wolof
        
        Args:
            wolof_text: Texte en wolof √† synth√©tiser
            output_filename: Nom du fichier de sortie (optionnel)
            voice_description: Description de la voix (optionnel)
            format: Format de sortie (wav, mp3)
            use_cache: Utiliser le cache si disponible
            
        Returns:
            Dictionnaire avec les informations du fichier audio g√©n√©r√©
        """
        if not wolof_text or not wolof_text.strip():
            raise ValueError("Le texte wolof ne peut pas √™tre vide")
        
        # Charger le mod√®le si n√©cessaire
        if not await self.load_model():
            raise RuntimeError("Impossible de charger le mod√®le ADIA TTS")
        
        voice_description = voice_description or self.default_voice_description
        wolof_text = wolof_text.strip()
        
        # V√©rifier le cache
        cache_key = self._get_cache_key(wolof_text, voice_description)
        
        # D√©duplication: si une g√©n√©ration pour la m√™me cl√© est en cours, attendre et r√©utiliser
        loop = asyncio.get_event_loop()
        with self._in_progress_lock:
            inprog = self._in_progress.get(cache_key)
            if inprog is not None:
                logger.info(f"‚è≥ Waiting for in-progress generation for key {cache_key}")
        if inprog is not None:
            # attendre que l'autre coroutine termine et retourner le r√©sultat cache
            try:
                await inprog
            except Exception:
                # l'op√©ration pr√©c√©dente a √©chou√©, continuer et lancer une nouvelle g√©n√©ration
                logger.warning("‚ö†Ô∏è Previous in-progress generation failed; retrying")
            with self._cache_lock:
                if cache_key in self._audio_cache:
                    logger.info(f"üì± Using cached audio after waiting: {self._audio_cache[cache_key]['file_path']}")
                    return self._audio_cache[cache_key]
        
        # Cr√©er un Future pour signaler aux requ√™tes concurrentes que la g√©n√©ration est en cours
        generation_future = loop.create_future()
        with self._in_progress_lock:
            self._in_progress[cache_key] = generation_future

        start_time = time.time()
        
        try:
            # G√©n√©rer le nom de fichier si non fourni
            if not output_filename:
                timestamp = int(time.time())
                safe_text = re.sub(r'[^a-zA-Z0-9_-]', '', wolof_text[:30])
                output_filename = f"wolof_audio_{safe_text}_{timestamp}.{format}"
            
            output_path = self.audio_output_dir / output_filename
            
            logger.info(f"üé§ Generating Wolof audio: '{wolof_text[:100]}...'")
            
            # 1. Diviser le texte en chunks
            chunks = self._split_text_into_chunks(wolof_text)
            
            # 2. G√©n√©rer l'audio pour chaque chunk en parall√®le
            audio_chunks = await self._generate_chunks_parallel(chunks, voice_description)
            
            # 3. Concat√©ner les chunks
            final_audio = self._concatenate_audio_chunks(audio_chunks)
            
            # 4. Sauvegarder le fichier
            if not self._save_audio_file(final_audio, output_path, format):
                raise RuntimeError("√âchec de la sauvegarde du fichier audio")
            
            generation_time = time.time() - start_time
            duration_seconds = len(final_audio) / self.audio_config["sample_rate"]
            
            result = {
                "success": True,
                "file_path": str(output_path),
                "filename": output_filename,
                "format": format,
                "duration_seconds": duration_seconds,
                "generation_time": generation_time,
                "chunks_count": len(chunks),
                "sample_rate": self.audio_config["sample_rate"],
                "file_size_bytes": output_path.stat().st_size if output_path.exists() else 0,
                "text_length": len(wolof_text),
                "voice_description": voice_description
            }
            
            # Mettre en cache
            if use_cache:
                with self._cache_lock:
                    self._audio_cache[cache_key] = result

            # R√©soudre la future pour les attendus concurrents
            if not generation_future.done():
                generation_future.set_result(result)

            logger.info(f"‚úÖ Wolof audio generated successfully:")
            logger.info(f"   File: {output_path}")
            logger.info(f"   Duration: {duration_seconds:.2f}s")
            logger.info(f"   Chunks: {len(chunks)}")
            logger.info(f"   Generation time: {generation_time:.2f}s")
            
            return result
            
        except Exception as e:
            error_msg = f"√âchec de la g√©n√©ration audio: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            if not generation_future.done():
                generation_future.set_exception(e)
            
            return {
                "success": False,
                "error": error_msg,
                "generation_time": time.time() - start_time,
                "text_length": len(wolof_text)
            }
        finally:
            # Nettoyage de l'√©tat in_progress
            with self._in_progress_lock:
                if cache_key in self._in_progress:
                    try:
                        del self._in_progress[cache_key]
                    except KeyError:
                        pass
    
    def clear_cache(self):
        """Vider le cache audio"""
        with self._cache_lock:
            self._audio_cache.clear()
        logger.info("üßπ Audio cache cleared")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """Obtenir des informations sur le cache"""
        with self._cache_lock:
            return {
                "cached_items": len(self._audio_cache),
                "cache_keys": list(self._audio_cache.keys())
            }
    
    def cleanup_old_files(self, max_age_hours: int = 24):
        """Nettoyer les anciens fichiers audio"""
        current_time = time.time()
        max_age_seconds = max_age_hours * 3600
        
        cleaned_count = 0
        
        for file_path in self.audio_output_dir.glob("*"):
            if file_path.is_file():
                file_age = current_time - file_path.stat().st_mtime
                if file_age > max_age_seconds:
                    try:
                        file_path.unlink()
                        cleaned_count += 1
                    except Exception as e:
                        logger.warning(f"Failed to delete old file {file_path}: {e}")
        
        logger.info(f"üßπ Cleaned up {cleaned_count} old audio files")
        return cleaned_count

# Instance globale du service TTS
tts_service = AdiaWolofTTSService()
